{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfbef4a-8e58-454b-83f5-f538a3042f68",
   "metadata": {},
   "source": [
    "## Importing dependencies\n",
    "\n",
    "### First Party\n",
    "\n",
    "Importing custom classes from the codebase:\n",
    "- **dataset**: helper _torch_ wrapper around _facades_ dataset.\n",
    "- **pix2pix**: implementation of _Pix2Pix_ architecture. Both generator and discriminator and the corresponding loss functions.\n",
    "\n",
    "### Third Party\n",
    "\n",
    "- **os**: to create directories, save weights.\n",
    "- **opencv**: image processing library, helps to load/save images.\n",
    "- **numpy**: nd arrays utilities, includes math and arithmetics.\n",
    "- **torch**: ML framework, extremelly friendly to GPU, the network architecture was specifically designed in torch.\n",
    "- **torchvision**: set of image transformations, like crop and flip. It is important to use _v2_ transforms as it can apply the same transformations to image and corresponding segmentation labels where stochastic operations used. \n",
    "- **matplotlib**: library to visualise data.\n",
    "- **tqdm**: visualises progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5732dac-7e81-4a5d-9ff6-f827510bb717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Party dependencies.\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1st Party dependencies.\n",
    "from dataset.facades_dataset import FacadesDataset\n",
    "from cyclegan.generator import Generator\n",
    "from cyclegan.discriminator import Discriminator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9005fc7-0a65-48de-9e24-69d10c9fdd2a",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "\n",
    "I decided to use the same pre-processing as was used for _Pix2Pix_ training. The pre-processing does not create too much disturbance in the data, so the model can learn something from it and at the same time it enriches the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7262c15b-dd0c-41a4-a383-704d1d5a6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the same transformations as were applied to Pix2Pix train dataset.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    # Resizing the 256×256 input images to 286×286.\n",
    "    transforms.Resize((286, 286)), \n",
    "    # Randomly cropping back to size 256×256.\n",
    "    transforms.RandomCrop(256),\n",
    "    # Mirroring.\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "default_transforms = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1ddd01-29b9-4504-9afb-8922f057a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "facades_train_dataset = FacadesDataset(root_dir='dataset/facades', split='train', transformations=train_transforms)\n",
    "facades_val_dataset = FacadesDataset(root_dir='dataset/facades', split='val', transformations=default_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(facades_train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(facades_val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845313b5-e749-4049-a0e7-4bd245901721",
   "metadata": {},
   "source": [
    "## Training cycle\n",
    "\n",
    "- **learning rate**: 0.0003\n",
    "- **lambda for cycle loss**: 10, quite big to pay a lot of attention to _x -> G(x) -> F(G(x))_ cycle.\n",
    "- **epochs**: 100, seems enough to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d47c75f-220a-46cd-80f0-b1a3f220f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    data_loader,\n",
    "    generator_x,\n",
    "    discriminator_x,\n",
    "    generator_y,\n",
    "    discriminator_y,\n",
    "    optimiser_generator,\n",
    "    optimiser_discriminator,\n",
    "    l1_loss_func, \n",
    "    mse_loss_func,\n",
    "    lambda_factor,\n",
    "    device):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    for y, x in tqdm(data_loader):\n",
    "        y = y.to(device)\n",
    "        x = x.to(device)\n",
    "\n",
    "        fake_x = generator_x(y)\n",
    "        d_x_real = discriminator_x(x)\n",
    "        d_x_fake = discriminator_x(fake_x.detach())\n",
    "        d_x_real_loss = mse_loss_func(d_x_real, torch.ones_like(d_x_real))\n",
    "        d_x_fake_loss = mse_loss_func(d_x_fake, torch.zeros_like(d_x_fake))\n",
    "        d_x_loss = d_x_real_loss + d_x_fake_loss\n",
    "\n",
    "        fake_y = generator_y(x)\n",
    "        d_y_real = discriminator_y(y)\n",
    "        d_y_fake = discriminator_y(fake_y.detach())\n",
    "        d_y_real_loss = mse_loss_func(d_y_real, torch.ones_like(d_y_real))\n",
    "        d_y_fake_loss = mse_loss_func(d_y_fake, torch.zeros_like(d_y_fake))\n",
    "        d_y_loss = d_y_real_loss + d_y_fake_loss\n",
    "\n",
    "        d_loss = (d_x_loss + d_y_loss) / 2\n",
    "\n",
    "        optimiser_discriminator.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimiser_discriminator.step()\n",
    "\n",
    "        # Adversarial losses.\n",
    "        d_x_fake = discriminator_x(fake_x)\n",
    "        d_y_fake = discriminator_y(fake_y)\n",
    "        loss_g_x = mse_loss_func(d_x_fake, torch.ones_like(d_x_fake))\n",
    "        loss_g_y = mse_loss_func(d_y_fake, torch.ones_like(d_y_fake))\n",
    "\n",
    "        # Cycle losses.\n",
    "        cycle_y = generator_y(fake_x)\n",
    "        cycle_x = generator_x(fake_y)\n",
    "        cycle_y_loss = l1_loss_func(y, cycle_y)\n",
    "        cycle_x_loss = l1_loss_func(x, cycle_x)\n",
    "\n",
    "        # Total generators loss.\n",
    "        g_loss = loss_g_y \\\n",
    "            + loss_g_x \\\n",
    "            + cycle_y_loss * lambda_factor \\\n",
    "            + cycle_x_loss * lambda_factor\n",
    "\n",
    "        optimiser_generator.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimiser_generator.step()\n",
    "\n",
    "        d_losses.append(d_loss.detach().cpu().item())\n",
    "        g_losses.append(g_loss.detach().cpu().item())\n",
    "\n",
    "    return np.mean(g_losses), np.mean(d_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151e283-c396-4b78-8190-261be0f4088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training: cuda was selected for training\n"
     ]
    }
   ],
   "source": [
    "# Setup.\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate = 3 * 1e-4\n",
    "lambda_cycle = 10\n",
    "epochs = 100\n",
    "\n",
    "print('Starting training:', device, 'was selected for training')\n",
    "\n",
    "# X -> Facade Segmentation.\n",
    "# Y -> Real facade image.\n",
    "generator_x = Generator(img_channels=3, num_residuals=9).to(device)\n",
    "discriminator_x = Discriminator(in_channels=3).to(device)\n",
    "generator_y = Generator(img_channels=3, num_residuals=9).to(device)\n",
    "discriminator_y = Discriminator(in_channels=3).to(device)\n",
    "\n",
    "optimiser_generator = torch.optim.Adam(\n",
    "    list(generator_x.parameters()) + list(generator_y.parameters()),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "optimiser_discriminator = torch.optim.Adam(\n",
    "    list(discriminator_x.parameters()) + list(discriminator_y.parameters()),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "l1_loss_function = nn.L1Loss()\n",
    "mse_loss_function = nn.MSELoss()\n",
    "\n",
    "generators_history = []\n",
    "discriminators_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    g_loss, d_loss = train_one_epoch(\n",
    "        train_dataloader,\n",
    "        generator_x,\n",
    "        discriminator_x,\n",
    "        generator_y,\n",
    "        discriminator_y,\n",
    "        optimiser_generator,\n",
    "        optimiser_discriminator,\n",
    "        l1_loss_function, \n",
    "        mse_loss_function,\n",
    "        lambda_cycle,\n",
    "        device)\n",
    "\n",
    "    generators_history.append(g_loss)\n",
    "    discriminators_history.append(d_loss)\n",
    "\n",
    "    print('Epoch:', epoch, 'generators loss:', g_loss, 'discriminators loss:', d_loss)\n",
    "\n",
    "    weights_dir = os.path.join('out', 'weights', 'cyclegan')\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "    torch.save(generator_x.state_dict(), os.path.join(weights_dir, f\"generator-x-{epoch:03d}-{g_loss:.3f}.pt\"))\n",
    "    torch.save(discriminator_x.state_dict(), os.path.join(weights_dir, f\"discriminator-x-{epoch:03d}-{d_loss:.3f}.pt\"))\n",
    "    torch.save(generator_y.state_dict(), os.path.join(weights_dir, f\"generator-y-{epoch:03d}-{g_loss:.3f}.pt\"))\n",
    "    torch.save(discriminator_y.state_dict(), os.path.join(weights_dir, f\"discriminator-y-{epoch:03d}-{d_loss:.3f}.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63047717-a30a-4ebd-9131-50c1c8d49a6c",
   "metadata": {},
   "source": [
    "## Training history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23074d-2434-4dc1-bbb0-e005409fe9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n",
    "\n",
    "ax[0].set_title(\"Generators loss history\")\n",
    "ax[0].plot(generators_history)\n",
    "\n",
    "ax[1].set_title(\"Discriminators loss history\")\n",
    "ax[1].plot(discriminators_history)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff816f1-7824-4972-8a2c-4d8818dc2315",
   "metadata": {},
   "source": [
    "The results seems plausible and quite real, though _Pix2Pix_ produces better results.\n",
    "At the same time we got 2 generators from _segmented facade_ to _real images_ and vice versa, while _Pix2Pix_ learns mapping in one direction only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf57de-619b-4017-8b2b-d06239232385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
